{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configrasyon ve Spark Baglantısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Administrator:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LENOVO'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkUser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark-shell'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_assert_on_driver',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getCheckpointDir',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Administrator:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"pyspark_uygulama\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temel DataFrame İşlemleri  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark.read.csv(\"diabetes.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pregnancies: int, Glucose: int, BloodPressure: int, SkinThickness: int, Insulin: int, BMI: double, DiabetesPedigreeFunction: double, Age: int, Outcome: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.cache()# veriyi hızlı kullanmak için işlemi ram'e taşıma işlemi yaparız,veriyi bir bakıma işlenbilir hale getirme işlemi yaptık  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python dataframe getirme işlemi \n",
    "import seaborn as sns \n",
    "df=sns.load_dataset(\"diamonds\")\n",
    "df=df.select_dtypes(include=[\"float64\",\"int64\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  table  price     x     y     z\n",
       "0   0.23   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python tarafında bazı fonksiyonları kullanamıyoruz\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Pregnancies=6, Glucose=148, BloodPressure=72, SkinThickness=35, Insulin=0, BMI=33.6, DiabetesPedigreeFunction=0.627, Age=50, Outcome=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.head()#yukarıdakinin verdiginin farklı bir gösterimini verdi bize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat    float64\n",
       "depth    float64\n",
       "table    float64\n",
       "price      int64\n",
       "x        float64\n",
       "y        float64\n",
       "z        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pregnancies', 'int'),\n",
       " ('Glucose', 'int'),\n",
       " ('BloodPressure', 'int'),\n",
       " ('SkinThickness', 'int'),\n",
       " ('Insulin', 'int'),\n",
       " ('BMI', 'double'),\n",
       " ('DiabetesPedigreeFunction', 'double'),\n",
       " ('Age', 'int'),\n",
       " ('Outcome', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes#yukarıdakinin verdiginin farklı bir gösterimini verdi bize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bafbee52cbd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#çalışmayacak fonksiyon ise,yani spark dataframe inde bazıları çalışmıyor buna bakmış olduk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;31m#size da hata verir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1657\u001b[0m         \"\"\"\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "#çalışmayacak fonksiyon ise,yani spark dataframe inde bazıları çalışmıyor buna bakmış olduk \n",
    "spark_df.ndim#size da hata verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame İşlemleri \n",
    "\n",
    "-pandas dataFrame'i paralel olarak çalışma özelligi yokturdur \n",
    "\n",
    "-layz mantıktadır,pandas da böyle degildir\n",
    "\n",
    "-inmutubledır bunu dönüştürme işlemine tabi tutarız,fakat pandas dataframe degiştirilebilir\n",
    "\n",
    "-pandas dataframe de veri manipülasyounun da daha fazla işlem yap imkanı vardır \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(2)#2 rakamı degitirilebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()#gözlem sayısını verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns#degişken isimlerine erişmek istersek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spark_df.columns)#yukarıdakinin sayısını bize verir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|               BMI|DiabetesPedigreeFunction|               Age|           Outcome|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "|  count|               768|              768|               768|               768|               768|               768|                     768|               768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.992578124999977|      0.4718763020833327|33.240885416666664|0.3489583333333333|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803| 7.884160320375441|       0.331328595012775|11.760231540678689| 0.476951377242799|\n",
      "|    min|                 0|                0|                 0|                 0|                 0|               0.0|                   0.078|                21|                 0|\n",
      "|    max|                17|              199|               122|                99|               846|              67.1|                    2.42|                81|                 1|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#özet istatistikleri almak istersek\n",
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          Glucose|\n",
      "+-------+-----------------+\n",
      "|  count|              768|\n",
      "|   mean|     120.89453125|\n",
      "| stddev|31.97261819513622|\n",
      "|    min|                0|\n",
      "|    max|              199|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#özet istatistikleri tek bir degişken için almak istersek\n",
    "spark_df.describe(\"Glucose\").show()#show u silersek bize yapı özellikerlini verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Glucose|Pregnancies|\n",
      "+-------+-----------+\n",
      "|    148|          6|\n",
      "|     85|          1|\n",
      "|    183|          8|\n",
      "|     89|          1|\n",
      "|    137|          0|\n",
      "+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#degişken seçme işlemleri ise \n",
    "spark_df.select(\"Glucose\",\"Pregnancies\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#degişken üzerinde eşsiz deger sayısına erişmek istersek\n",
    "spark_df.select(\"Glucose\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Glucose|\n",
      "+-------+\n",
      "|    148|\n",
      "|     85|\n",
      "|    137|\n",
      "|     65|\n",
      "|    133|\n",
      "|     78|\n",
      "|    155|\n",
      "|    108|\n",
      "|    193|\n",
      "|    115|\n",
      "|    126|\n",
      "|    101|\n",
      "|     81|\n",
      "|    183|\n",
      "|     76|\n",
      "|    159|\n",
      "|     44|\n",
      "|    103|\n",
      "|     91|\n",
      "|    128|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#veri setinin içindeki çoklama verilerini silmek istersek \n",
    "spark_df.select(\"Glucose\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|Outcome_Pregnancies|  0|  1| 10| 11| 12| 13| 14| 15| 17|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "+-------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|                  1| 38| 29| 10|  7|  4|  5|  2|  1|  1| 19| 27| 23| 21| 16| 25| 22| 18|\n",
      "|                  0| 73|106| 14|  4|  5|  5|  0|  0|  0| 84| 48| 45| 36| 34| 20| 16| 10|\n",
      "+-------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iki degişken arasında croostab oluşturmak istersek\n",
    "spark_df.crosstab(\"Outcome\",\"Pregnancies\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eksik gözlem sayısını bulmak için\n",
    "spark_df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|\n",
      "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|\n",
      "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|      1|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|\n",
      "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|      0|\n",
      "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|      1|\n",
      "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|      1|\n",
      "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|      1|\n",
      "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|      1|\n",
      "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|      1|\n",
      "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|      0|\n",
      "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#eksik gözlemlerden kurtulmak istersek \n",
    "spark_df.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gözlem seçme işlemlerini ele alırsak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.filter(spark_df.Age>40).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Outcome|count|\n",
      "+-------+-----+\n",
      "|      1|  268|\n",
      "|      0|  500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gruplama işlemi yapmak istersek\n",
    "spark_df.groupby(\"Outcome\").count().show()\n",
    "#show'u kapatırsek göstermez layz yapı dedigimiz bu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Outcome|         avg(BMI)|\n",
      "+-------+-----------------+\n",
      "|      1|35.14253731343278|\n",
      "|      0|30.30419999999996|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gruplara göre bir ortalama işlemi alırsak \n",
    "spark_df.groupby(\"Outcome\").agg({\"BMI\":\"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "| BMI|yeni_degisken|\n",
      "+----+-------------+\n",
      "|33.6|         16.8|\n",
      "|26.6|         13.3|\n",
      "|23.3|        11.65|\n",
      "|28.1|        14.05|\n",
      "|43.1|        21.55|\n",
      "|25.6|         12.8|\n",
      "|31.0|         15.5|\n",
      "|35.3|        17.65|\n",
      "|30.5|        15.25|\n",
      "| 0.0|          0.0|\n",
      "|37.6|         18.8|\n",
      "|38.0|         19.0|\n",
      "|27.1|        13.55|\n",
      "|30.1|        15.05|\n",
      "|25.8|         12.9|\n",
      "|30.0|         15.0|\n",
      "|45.8|         22.9|\n",
      "|29.6|         14.8|\n",
      "|43.3|        21.65|\n",
      "|34.6|         17.3|\n",
      "+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#degişken ekleme işlemi,bu var olan bir degişken üzerinden yapıldı\n",
    "spark_df.withColumn(\"yeni_degisken\",spark_df.BMI/2).select(\"BMI\",\"yeni_degisken\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'bagimli_degisken']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#degişken ismini nasıl degiştiririz\n",
    "#aşagıda Outcome bagimli_degisken şeklinde degiştirdik\n",
    "spark_df.withColumnRenamed(\"Outcome\",\"bagimli_degisken\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bir degişkeni silmek istersek,Insulin'i sildik \n",
    "spark_df.drop(\"Insulin\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Outcome|count|\n",
      "+-------+-----+\n",
      "|      1|  268|\n",
      "|      0|  500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ram'a çektigimiz işlemleri geri pandasa çekmek istersek\n",
    "spark_df.groupby(\"Outcome\").count().show()\n",
    "#sonuna [0:1,0:1]şeklinde yapınca erişim saglayamdık bunun yerine toPandas kullanıcaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=spark_df.groupby(\"Outcome\").count().toPandas()#spark dataFrame'i pandas dataframe çevirdik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outcome  count\n",
       "0        1    268\n",
       "1        0    500"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outcome\n",
       "0        1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#şimdi ise [0:1,0:1] şeklide erişim saglaybiliriz\n",
    "a.iloc[0:1,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL İşlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.registerTempTable(\"table_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         | table_df|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()#tabloları bu şeklilde gözlemliyoruz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Glucose|\n",
      "+-------+\n",
      "|    148|\n",
      "|     85|\n",
      "|    183|\n",
      "|     89|\n",
      "|    137|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Glucose from table_df\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o249.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 62) (Administrator executor driver): java.io.FileNotFoundException: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-f57337ae-4ebf-4ab9-9f86-418acb9da042\\37\\temp_shuffle_07a11c44-4fe9-4766-8387-28fcb6046e24 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-f57337ae-4ebf-4ab9-9f86-418acb9da042\\37\\temp_shuffle_07a11c44-4fe9-4766-8387-28fcb6046e24 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-5fd1bd874578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select Outcome, mean(Glucose) from table_df group by Outcome\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o249.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 62) (Administrator executor driver): java.io.FileNotFoundException: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-f57337ae-4ebf-4ab9-9f86-418acb9da042\\37\\temp_shuffle_07a11c44-4fe9-4766-8387-28fcb6046e24 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-f57337ae-4ebf-4ab9-9f86-418acb9da042\\37\\temp_shuffle_07a11c44-4fe9-4766-8387-28fcb6046e24 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\r\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Outcome, mean(Glucose) from table_df group by Outcome\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Büyük Veri Görselleştirme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-5257294b57b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Outcome\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutcome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#spark_df olarak çalıştırdıgımız için hata verdi bize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[1;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3142\u001b[0m             ax=None, **kwargs):\n\u001b[0;32m   3143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3144\u001b[1;33m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[0;32m   3145\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m                           \u001b[0morient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1600\u001b[0m                  errwidth, capsize, dodge):\n\u001b[0;32m   1601\u001b[0m         \u001b[1;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1602\u001b[1;33m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[0;32m   1603\u001b[0m                                  order, hue_order, units)\n\u001b[0;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;31m# See if we need to get variables from `data`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mhue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1657\u001b[0m         \"\"\"\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "sns.barplot(x=\"Outcome\",y=spark_df.Outcome.index,data=spark_df)\n",
    "#spark_df olarak çalıştırdıgımız için hata verdi bize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "büyük veri görselleştirme aslında 2 basamaktan oluşuyor,\n",
    "\n",
    "1.veriyi spark tarafında agreegate etmek,dönüştürme işlemi yapmak\n",
    "\n",
    "2.sonra toPandas'a çekip alışık oludugumuz işlemlere tabi tutarız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf=spark_df.toPandas()\n",
    "sdf.head()\n",
    "#burada veri küçük oldugu için spark dataFrame'ini çevirdik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Outcome'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARiklEQVR4nO3de7BdZXnH8e/PhIv1Bsgp0iQ0jMY6WDXaU9Sxf1ioirTTYEcdGEdSy0zsFDtarYpOp2qnzKhVqfTCTCxIdKiKt5I69EIBa7WCHjCGm9YjiiSTkINcxFppE57+cd7oJpzk7HOPb76fmTN7rWe9a69nZ878zuJl7bVSVUiS+vKopW5AkjT/DHdJ6pDhLkkdMtwlqUOGuyR1aPlSNwBw7LHH1urVq5e6DUn6mXLDDTfcXVUjU207KMJ99erVjI2NLXUbkvQzJckd+9vmtIwkdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQwfFl5g0f97ylrewc+dOnvSkJ/He9753qduRtEQM987s3LmT7du3L3UbkpaY0zKS1KGhwz3JsiRfS/K5tn5ikuuTjCf5RJLDW/2Itj7etq9eoN4lSfsxkzP31wO3Day/B7igqp4C3Auc0+rnAPe2+gVtnCRpEQ0V7klWAr8J/F1bD3AK8Kk2ZBNwRlte19Zp209t4yVJi2TYM/e/BN4CPNTWnwjcV1W72/o2YEVbXgHcCdC239/GP0ySDUnGkoxNTEzMrntJ0pSmDfckvwXsqqob5vPAVbWxqkaranRkZMp7zUuSZmmYSyFfAPx2ktOBI4HHAx8EjkqyvJ2drwT2Xn+3HVgFbEuyHHgC8P1571zSzxS/g7G4pj1zr6q3VdXKqloNnAlcU1WvAq4FXt6GrQeuaMub2zpt+zVVVfPataSfOXu/g7Fz586lbuWQMJfr3N8KvDHJOJNz6he3+sXAE1v9jcB5c2tRkjRTM/qGalV9Hvh8W74dOHmKMT8GXjEPvc3Ir7z5I4t9yIPS4+5+gGXA9+5+wH8T4Ia/OHupW5CWhN9QlaQOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDvkMVWmBfe/PnrHULRwUdt9zDLCc3ffc4b8JcMKf3rSg72+4d+ahwx/zsFdJhybDvTP/vebFS92CpIOAc+6S1CHDXZI6ZLhLUocMd0nq0DAPyD4yyVeSfD3JLUne1eqXJvlOki3tZ22rJ8mFScaTbE3ynAX+DJKkfQxztcyDwClV9cMkhwFfTPJPbdubq+pT+4x/KbCm/TwXuKi9SpIWyTAPyK6q+mFbPaz9HOiB1+uAj7T9rgOOSnL83FuVJA1rqDn3JMuSbAF2AVdV1fVt0/lt6uWCJEe02grgzoHdt7Xavu+5IclYkrGJiYnZfwJJ0iMMFe5Vtaeq1gIrgZOT/DLwNuBpwK8CxwBvncmBq2pjVY1W1ejIyMjMupYkHdCMrpapqvuAa4HTqmpHm3p5EPgwcHIbth1YNbDbylaTJC2SYa6WGUlyVFt+NPAi4Bt759GTBDgDuLntshk4u1018zzg/qrasQC9S5L2Y5irZY4HNiVZxuQfg8ur6nNJrkkyAgTYAvx+G38lcDowDvwIeM28dy1JOqBpw72qtgLPnqJ+yn7GF3Du3FuT1JNjj3wI2N1etdC8K6SkRfHHz7xvqVs4pHj7AUnqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjo0zGP2jkzylSRfT3JLkne1+olJrk8ynuQTSQ5v9SPa+njbvnqBP4MkaR/DnLk/CJxSVc8C1gKntWejvge4oKqeAtwLnNPGnwPc2+oXtHGSpEU0bbjXpB+21cPaTwGnAJ9q9U1MPiQbYF1bp20/tT1EW5K0SIaac0+yLMkWYBdwFfBt4L6q2t2GbANWtOUVwJ0Abfv9wBPnsWdJ0jSGCveq2lNVa4GVwMnA0+Z64CQbkowlGZuYmJjr20mSBszoapmqug+4Fng+cFSSvQ/YXglsb8vbgVUAbfsTgO9P8V4bq2q0qkZHRkZm170kaUrDXC0zkuSotvxo4EXAbUyG/MvbsPXAFW15c1unbb+mqmoee5YkTWP59EM4HtiUZBmTfwwur6rPJbkV+HiSPwe+Blzcxl8MfDTJOHAPcOYC9C1JOoBpw72qtgLPnqJ+O5Pz7/vWfwy8Yl66kyTNit9QlaQOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4N8wzVVUmuTXJrkluSvL7V35lke5It7ef0gX3elmQ8yTeTvGQhP4Ak6ZGGeYbqbuBNVXVjkscBNyS5qm27oKreNzg4yUlMPjf16cAvAP+W5KlVtWc+G5ck7d+0Z+5VtaOqbmzLDwC3ASsOsMs64ONV9WBVfQcYZ4pnrUqSFs6M5tyTrGbyYdnXt9LrkmxNckmSo1ttBXDnwG7bmOKPQZINScaSjE1MTMy8c0nSfg0d7kkeC3waeENV/QC4CHgysBbYAbx/Jgeuqo1VNVpVoyMjIzPZVZI0jaHCPclhTAb7ZVX1GYCququq9lTVQ8CH+OnUy3Zg1cDuK1tNkrRIhrlaJsDFwG1V9YGB+vEDw14G3NyWNwNnJjkiyYnAGuAr89eyJGk6w1wt8wLg1cBNSba02tuBs5KsBQr4LvBagKq6JcnlwK1MXmlzrlfKSNLimjbcq+qLQKbYdOUB9jkfOH8OfUmS5sBvqEpShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOjTMY/ZWJbk2ya1Jbkny+lY/JslVSb7VXo9u9SS5MMl4kq1JnrPQH0KS9HDDnLnvBt5UVScBzwPOTXIScB5wdVWtAa5u6wAvZfK5qWuADcBF8961JOmApg33qtpRVTe25QeA24AVwDpgUxu2CTijLa8DPlKTrgOO2udh2pKkBTajOfckq4FnA9cDx1XVjrZpJ3BcW14B3Dmw27ZW2/e9NiQZSzI2MTEx074lSQcwdLgneSzwaeANVfWDwW1VVUDN5MBVtbGqRqtqdGRkZCa7SpKmMVS4JzmMyWC/rKo+08p37Z1uaa+7Wn07sGpg95WtJklaJMNcLRPgYuC2qvrAwKbNwPq2vB64YqB+drtq5nnA/QPTN5KkRbB8iDEvAF4N3JRkS6u9HXg3cHmSc4A7gFe2bVcCpwPjwI+A18xnw5Kk6U0b7lX1RSD72XzqFOMLOHeOfUmS5sBvqEpShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOjTMY/YuSbIryc0DtXcm2Z5kS/s5fWDb25KMJ/lmkpcsVOOSpP0b5sz9UuC0KeoXVNXa9nMlQJKTgDOBp7d9/jbJsvlqVpI0nGnDvaq+ANwz5PutAz5eVQ9W1XeYfI7qyXPoT5I0C3OZc39dkq1t2uboVlsB3DkwZlurPUKSDUnGkoxNTEzMoQ1J0r5mG+4XAU8G1gI7gPfP9A2qamNVjVbV6MjIyCzbkCRNZVbhXlV3VdWeqnoI+BA/nXrZDqwaGLqy1SRJi2hW4Z7k+IHVlwF7r6TZDJyZ5IgkJwJrgK/MrUVJ0kwtn25Ako8BLwSOTbINeAfwwiRrgQK+C7wWoKpuSXI5cCuwGzi3qvYsSOeSpP2aNtyr6qwpyhcfYPz5wPlzaUqSNDd+Q1WSOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1KFpwz3JJUl2Jbl5oHZMkquSfKu9Ht3qSXJhkvEkW5M8ZyGblyRNbZgz90uB0/apnQdcXVVrgKvbOsBLmXxu6hpgA3DR/LQpSZqJacO9qr4A3LNPeR2wqS1vAs4YqH+kJl0HHLXPw7QlSYtgtnPux1XVjra8EziuLa8A7hwYt63VHiHJhiRjScYmJiZm2YYkaSpz/h+qVVVAzWK/jVU1WlWjIyMjc21DkjRgtuF+197plva6q9W3A6sGxq1sNUnSIpptuG8G1rfl9cAVA/Wz21UzzwPuH5i+kSQtkuXTDUjyMeCFwLFJtgHvAN4NXJ7kHOAO4JVt+JXA6cA48CPgNQvQsyRpGtOGe1WdtZ9Np04xtoBz59qUJGlu/IaqJHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalD0z6s40CSfBd4ANgD7K6q0STHAJ8AVgPfBV5ZVffOrU1J0kzMx5n7r1fV2qoabevnAVdX1Rrg6rYuSVpECzEtsw7Y1JY3AWcswDEkSQcw13Av4F+T3JBkQ6sdV1U72vJO4Lg5HkOSNENzmnMHfq2qtif5eeCqJN8Y3FhVlaSm2rH9MdgAcMIJJ8yxDUnSoDmduVfV9va6C/gscDJwV5LjAdrrrv3su7GqRqtqdGRkZC5tSJL2MetwT/KYJI/buwy8GLgZ2Aysb8PWA1fMtUlJ0szMZVrmOOCzSfa+z99X1T8n+SpweZJzgDuAV869TUnSTMw63KvqduBZU9S/D5w6l6YkSXPjN1QlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQwsW7klOS/LNJONJzluo40iSHmlBwj3JMuBvgJcCJwFnJTlpIY4lSXqkhTpzPxkYr6rbq+p/gY8D6xboWJKkfcz6AdnTWAHcObC+DXju4IAkG4ANbfWHSb65QL0cio4F7l7qJg4Ged/6pW5BD+fv5l7vyHy8yy/ub8NChfu0qmojsHGpjt+zJGNVNbrUfUj78ndz8SzUtMx2YNXA+spWkyQtgoUK968Ca5KcmORw4Exg8wIdS5K0jwWZlqmq3UleB/wLsAy4pKpuWYhjaUpOd+lg5e/mIklVLXUPkqR55jdUJalDhrskdchw74i3fNDBKsklSXYluXmpezlUGO6d8JYPOshdCpy21E0cSgz3fnjLBx20quoLwD1L3cehxHDvx1S3fFixRL1IWmKGuyR1yHDvh7d8kPQThns/vOWDpJ8w3DtRVbuBvbd8uA243Fs+6GCR5GPAl4FfSrItyTlL3VPvvP2AJHXIM3dJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7upGkpVJrkjyrSTfTvLBds3/gfZ5+2L1Jy0mw11dSBLgM8A/VNUa4KnAY4Hzp9nVcFeXDHf14hTgx1X1YYCq2gP8EfB7Sf4gyV/vHZjkc0lemOTdwKOTbElyWdt2dpKtSb6e5KOttjrJNa1+dZITWv3SJBcluS7J7e09L0lyW5JLB4734iRfTnJjkk8meeyi/avokGW4qxdPB24YLFTVD4DvsZ8HwVfVecD/VNXaqnpVkqcDfwKcUlXPAl7fhv4VsKmqnglcBlw48DZHA89n8g/JZuCC1sszkqxNcmx7z9+oqucAY8Ab5+MDSwcy5S+9dIg6BfhkVd0NUFV77z/+fOB32vJHgfcO7POPVVVJbgLuqqqbAJLcAqxm8gZuJwFfmpw54nAmv4YvLSjDXb24FXj5YCHJ44ETgPt4+H+lHjmPx32wvT40sLx3fTmwB7iqqs6ax2NK03JaRr24Gvi5JGfDTx47+H4mH+92O7A2yaOSrGLyqVV7/V+Sw9ryNcArkjyxvccxrf6fTN5lE+BVwH/MoK/rgBckeUp7z8ckeepMP5w0U4a7ulCTd8B7GZPh/C3gv4AfM3k1zJeA7zB5dn8hcOPArhuBrUkua3fRPB/49yRfBz7Qxvwh8JokW4FX89O5+GH6mgB+F/hY2//LwNNm+zmlYXlXSEnqkGfuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR16P8B4q+bxbEfExMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=\"Outcome\",y=sdf.Outcome.index,data=sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
